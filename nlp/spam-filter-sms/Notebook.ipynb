{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple SMS Spam Filter in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda\n",
    "\n",
    "This project aims at implementing several methods for SPAM-filtering in Python, on a dataset containing SMS data. It is developed in the script *sms_spam_filter.py*\n",
    "\n",
    "It uses a [dataset](https://raw.githubusercontent.com/justmarkham/DAT8/master/data/sms.tsv) of 5,572 labelled SMS, and a Multinomial Naive Bayes model for classification.\n",
    "\n",
    "This notebook is organised as follow:\n",
    "* [1) Reading the SMS data](#1-reading-the-sms-data)\n",
    "* [2) Learning to predict \"spamminess\"](#2-learning-to-predict-spamminess)\n",
    "* [3) Analyzing the results](#3-analyzing-the-results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Reading the SMS data <a class=\"anchor\" id=\"1-reading-the-sms-data\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first section focuses on reading the data, going through it a bit and preparing the datasets we are going to use for the purpose of this study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's load the data from an online TSV file\n",
    "url = 'https://raw.githubusercontent.com/justmarkham/DAT8/master/data/sms.tsv'\n",
    "sms = pd.read_table(url, header=None, names=['label', 'message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spam</td>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spam</td>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
       "5  spam  FreeMsg Hey there darling it's been 3 week's n...\n",
       "6   ham  Even my brother is not like to speak with me. ...\n",
       "7   ham  As per your request 'Melle Melle (Oru Minnamin...\n",
       "8  spam  WINNER!! As a valued network customer you have...\n",
       "9  spam  Had your mobile 11 months or more? U R entitle..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check how the data looks\n",
    "sms.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     4825\n",
       "spam     747\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check the balance of the dataset\n",
    "sms.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset seems to be very unbalanced. Depending on the chosen model, this might be an issue during the learning process (as the null model might be a relatively precise model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  target\n",
       "0   ham  Go until jurong point, crazy.. Available only ...       0\n",
       "1   ham                      Ok lar... Joking wif u oni...       0\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...       1\n",
       "3   ham  U dun say so early hor... U c already then say...       0\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...       0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let us create a proper target column. 1 will be spam, since this is what we are trying to detect\n",
    "sms['target'] = sms.label.map({'ham':0, 'spam':1})\n",
    "sms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572,) (5572,)\n"
     ]
    }
   ],
   "source": [
    "# Let us now create our overall dataset\n",
    "X = sms.message\n",
    "y = sms.target\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both have the same length, which is normal, and are one-dimensional. This is due to the fact that for now, the data is not yet vectorized and each observation of X is one unique string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But first, let us split our dataset into two sets:\n",
    "* A training set, on which we will train our model (using cross-validation first to assess its performance and try to optimize parameters, before training the chosen model on the whole dataset)\n",
    "* A valid set, on which we will assess the performance of our best model, once our model is optimized and trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Learning to predict \"spamminess\" <a class=\"anchor\" id=\"2-learning-to-predict-spamminess\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to do is to vectorize the datasets (i.e. get a numerical matrix out of this string one-dimensional object). In order to do so, we will use CountVectorizer, which will create a sparse document-term matrix (DTM) containing the observations (\"documents\") on the first dimension, the tokens (\"terms\") on the second dimension, and the number of appearances of each token in each observation as values in the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB as MNB\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first difficulty here is the following: our vectorizer vect must be fitted only on the training set, so that the document-term matrix of the training set does not contain any information of the testing set (i.e. so that the frequencies of the training DTM do not include testing observations).\n",
    "\n",
    "How to use cross-validation, then? The solution is pipelines. They enable to pass a pipeline(vectorizer, model) which will be fitted on the training set only, before being applied on the testing set, for every iteration of the cross-validation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vect = CountVectorizer(max_features=4000, max_df=0.4)\n",
    "mnb = MNB()\n",
    "\n",
    "pipeline = make_pipeline(vect, mnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9851037347738929"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=5)\n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters chosen above have been adjusted by trial and error, trying to improve the overall mean score obtained through cross-validation. Other parameters have been tested, such as *stop_words*, *mean_df* and *ngram_range*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can just fit the pipeline on the whole train set. This simple operation does the following things:\n",
    "* Fits the vectorizer on the training set\n",
    "* Transforms the training set with the fitted vectorizer\n",
    "* Fits the model on the training set with the given targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99103139013452912"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y_valid, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Our model gets a prediction accuracy of 99.1%, which is very satisfying… 99% of the SMS tested were accurately predicted spam or ham."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Analyzing the results <a class=\"anchor\" id=\"3-analyzing-the-results\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to need to access to our vectorizer's attributes for going further in the analyses of the results, so let's repeat the previous process without using the pipeline (this is also the opportunity to notice how convenient the pipeline is)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vect = CountVectorizer(max_df=0.4, max_features=4000)\n",
    "mnb = MNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_dtm = vect.fit_transform(X_train, y_train)\n",
    "mnb.fit(X_train_dtm, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99103139013452912"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid_dtm = vect.transform(X_valid)\n",
    "y_pred = mnb.predict(X_valid_dtm)\n",
    "\n",
    "metrics.accuracy_score(y_valid, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.a) Measuring accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's check out the ROC AUC score, which can be seen as a measure of how efficient the model is to rank the items per probability of belonging to each class.\n",
    "This is often a good indicator when you want to get the predictions per order of probability (*e.g.* if you want to know those which are the most likely to be class 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9746408894136166"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.roc_auc_score(y_valid, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check confusion matrix. This could be a useful metric if we decided that one mistake should weigh more than another.\n",
    "\n",
    "For example, one could rather have more actual spam predicted ham, and less actual ham predicted spam (in order not to miss information): it would thus be needed to check this confusion metric (or the false positive rate) during the model optimisation phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[965,   3],\n",
       "       [  7, 140]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(y_valid, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be interesting to know on which SMS our model failed to predict accurately the class. To do so, let's check out the actual false positives and false negatives of our prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "574               Waiting for your call.\n",
       "3375             Also andros ice etc etc\n",
       "45      No calls..messages..missed calls\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# False positives:\n",
    "pd.set_option(\"display.max_colwidth\",150)\n",
    "X_valid[y_valid < y_pred].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3132    LookAtMe!: Thanks for your purchase of a video clip from LookAtMe!, you've been charged 35p. Think you can do better? Why not send a video in a MM...\n",
       "5         FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, £1.50 to rcv\n",
       "3530    Xmas & New Years Eve tickets are now on sale from the club, during the day from 10am till 8pm, and on Thurs, Fri & Sat night this week. They're se...\n",
       "1875                                                                     Would you like to see my XXX pics they are so hot they were nearly banned in the uk!\n",
       "1893    CALL 09090900040 & LISTEN TO EXTREME DIRTY LIVE CHAT GOING ON IN THE OFFICE RIGHT NOW TOTAL PRIVACY NO ONE KNOWS YOUR [sic] LISTENING 60P MIN 24/7...\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# False negatives:\n",
    "pd.set_option(\"display.max_colwidth\",150)\n",
    "X_valid[y_valid > y_pred].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can notice false positives are all very short messages, and my guess would be that they have all one \"spammy\" word among those few words (\"call\", for example), which makes them get a spam prediction, since there are not enough \"hammy\" words to counter-balance this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.b) Spamminess of each token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now try to compute the \"spamminess\" and \"hamminess\" we were talking about in the previous section.\n",
    "\n",
    "To do so, one can take a look at the frequency of appearance of every word in spam content, and in ham content. Then, the ratio spam_frequency by ham_frequency could be taken as a measurement of spamminess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# store the vocabulary of X_train\n",
    "X_train_tokens = vect.get_feature_names()\n",
    "len(X_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.,   0.,   0., ...,   0.,   2.,   1.],\n",
       "       [  5.,  24.,   2., ...,   6.,   0.,   1.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes counts the number of times each token appears in each class\n",
    "mnb.feature_count_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0., ...,  0.,  2.,  1.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of times each token appears across all HAM messages\n",
    "ham_token_count = mnb.feature_count_[0, :]\n",
    "ham_token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  5.,  24.,   2., ...,   6.,   0.,   1.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of times each token appears across all SPAM messages\n",
    "spam_token_count = mnb.feature_count_[1, :]\n",
    "spam_token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ham</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>respect</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>senses</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lol</th>\n",
       "      <td>64.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>affair</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>haven</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ham  spam\n",
       "token              \n",
       "respect   4.0   0.0\n",
       "senses    1.0   0.0\n",
       "lol      64.0   0.0\n",
       "affair    2.0   0.0\n",
       "haven    19.0   0.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a DataFrame of tokens with their separate ham and spam counts\n",
    "tokens = pd.DataFrame({'token':X_train_tokens, 'ham':ham_token_count, 'spam':spam_token_count}).set_index('token')\n",
    "tokens.sample(5, random_state=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can use this to calculate the \"spamminess\" of each token, we need to avoid dividing by zero and take care of the class imbalance (by using frequencies rather than counts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add 1 to ham and spam counts to avoid dividing by 0\n",
    "tokens['ham'] = tokens.ham + 1\n",
    "tokens['spam'] = tokens.spam + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ham</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>respect</th>\n",
       "      <td>0.001296</td>\n",
       "      <td>0.001667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>senses</th>\n",
       "      <td>0.000519</td>\n",
       "      <td>0.001667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lol</th>\n",
       "      <td>0.016852</td>\n",
       "      <td>0.001667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>affair</th>\n",
       "      <td>0.000778</td>\n",
       "      <td>0.001667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>haven</th>\n",
       "      <td>0.005185</td>\n",
       "      <td>0.001667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ham      spam\n",
       "token                      \n",
       "respect  0.001296  0.001667\n",
       "senses   0.000519  0.001667\n",
       "lol      0.016852  0.001667\n",
       "affair   0.000778  0.001667\n",
       "haven    0.005185  0.001667"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the ham and spam counts into frequencies\n",
    "tokens['ham'] = tokens.ham / mnb.class_count_[0]\n",
    "tokens['spam'] = tokens.spam / mnb.class_count_[1]\n",
    "tokens.sample(5, random_state=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ham</th>\n",
       "      <th>spam</th>\n",
       "      <th>spamminess</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>respect</th>\n",
       "      <td>0.001296</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>1.285667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>senses</th>\n",
       "      <td>0.000519</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>3.214167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lol</th>\n",
       "      <td>0.016852</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.098897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>affair</th>\n",
       "      <td>0.000778</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>2.142778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>haven</th>\n",
       "      <td>0.005185</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.321417</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ham      spam  spamminess\n",
       "token                                  \n",
       "respect  0.001296  0.001667    1.285667\n",
       "senses   0.000519  0.001667    3.214167\n",
       "lol      0.016852  0.001667    0.098897\n",
       "affair   0.000778  0.001667    2.142778\n",
       "haven    0.005185  0.001667    0.321417"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the spamminess (spam_frequency/ham_frequency)\n",
    "tokens['spamminess'] = tokens.spam / tokens.ham\n",
    "tokens.sample(5, random_state=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, one can look at the following list, sorted by descending spamminess... And it all seems quite understandable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ham</th>\n",
       "      <th>spam</th>\n",
       "      <th>spamminess</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>claim</th>\n",
       "      <td>0.000259</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>578.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prize</th>\n",
       "      <td>0.000259</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>514.266667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150p</th>\n",
       "      <td>0.000259</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>347.130000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tone</th>\n",
       "      <td>0.000259</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>321.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>guaranteed</th>\n",
       "      <td>0.000259</td>\n",
       "      <td>0.076667</td>\n",
       "      <td>295.703333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.000259</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>289.275000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>www</th>\n",
       "      <td>0.000519</td>\n",
       "      <td>0.135000</td>\n",
       "      <td>260.347500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cs</th>\n",
       "      <td>0.000259</td>\n",
       "      <td>0.061667</td>\n",
       "      <td>237.848333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>0.000259</td>\n",
       "      <td>0.058333</td>\n",
       "      <td>224.991667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.000259</td>\n",
       "      <td>0.056667</td>\n",
       "      <td>218.563333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>awarded</th>\n",
       "      <td>0.000259</td>\n",
       "      <td>0.053333</td>\n",
       "      <td>205.706667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uk</th>\n",
       "      <td>0.000519</td>\n",
       "      <td>0.103333</td>\n",
       "      <td>199.278333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150ppm</th>\n",
       "      <td>0.000259</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>192.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000</th>\n",
       "      <td>0.000259</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>160.708333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ringtone</th>\n",
       "      <td>0.000259</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>160.708333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>co</th>\n",
       "      <td>0.000519</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>154.280000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mob</th>\n",
       "      <td>0.000259</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>154.280000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>collection</th>\n",
       "      <td>0.000259</td>\n",
       "      <td>0.038333</td>\n",
       "      <td>147.851667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>0.000259</td>\n",
       "      <td>0.036667</td>\n",
       "      <td>141.423333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>valid</th>\n",
       "      <td>0.000259</td>\n",
       "      <td>0.036667</td>\n",
       "      <td>141.423333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10p</th>\n",
       "      <td>0.000259</td>\n",
       "      <td>0.036667</td>\n",
       "      <td>141.423333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5000</th>\n",
       "      <td>0.000259</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>128.566667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weekly</th>\n",
       "      <td>0.000259</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>128.566667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8007</th>\n",
       "      <td>0.000259</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>128.566667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000519</td>\n",
       "      <td>0.065000</td>\n",
       "      <td>125.352500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entry</th>\n",
       "      <td>0.000259</td>\n",
       "      <td>0.031667</td>\n",
       "      <td>122.138333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>national</th>\n",
       "      <td>0.000259</td>\n",
       "      <td>0.031667</td>\n",
       "      <td>122.138333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sae</th>\n",
       "      <td>0.000259</td>\n",
       "      <td>0.031667</td>\n",
       "      <td>122.138333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tones</th>\n",
       "      <td>0.000259</td>\n",
       "      <td>0.031667</td>\n",
       "      <td>122.138333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>land</th>\n",
       "      <td>0.000259</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>115.710000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>went</th>\n",
       "      <td>0.012186</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.136773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gonna</th>\n",
       "      <td>0.012186</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.136773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dun</th>\n",
       "      <td>0.012445</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.133924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oh</th>\n",
       "      <td>0.024890</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>0.133924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>but</th>\n",
       "      <td>0.090744</td>\n",
       "      <td>0.011667</td>\n",
       "      <td>0.128567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>told</th>\n",
       "      <td>0.013482</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.123622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gud</th>\n",
       "      <td>0.013741</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.121289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feel</th>\n",
       "      <td>0.014519</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.114792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>something</th>\n",
       "      <td>0.014519</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.114792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cos</th>\n",
       "      <td>0.015038</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.110833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sure</th>\n",
       "      <td>0.015297</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.108955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>said</th>\n",
       "      <td>0.016852</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.098897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lol</th>\n",
       "      <td>0.016852</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.098897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>morning</th>\n",
       "      <td>0.017112</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.097399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yeah</th>\n",
       "      <td>0.017112</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.097399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amp</th>\n",
       "      <td>0.017630</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.094534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anything</th>\n",
       "      <td>0.017890</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.093164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doing</th>\n",
       "      <td>0.018927</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.088059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>my</th>\n",
       "      <td>0.151932</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.087759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>already</th>\n",
       "      <td>0.019445</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.085711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ask</th>\n",
       "      <td>0.019704</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.084583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>way</th>\n",
       "      <td>0.021001</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.079362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>come</th>\n",
       "      <td>0.047446</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>0.070255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>later</th>\n",
       "      <td>0.030075</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.055417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>da</th>\n",
       "      <td>0.032409</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.051427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lor</th>\n",
       "      <td>0.032927</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.050617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>she</th>\n",
       "      <td>0.036038</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.046247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>he</th>\n",
       "      <td>0.047187</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.035321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lt</th>\n",
       "      <td>0.064817</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.025713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gt</th>\n",
       "      <td>0.065595</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.025408</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ham      spam  spamminess\n",
       "token                                     \n",
       "claim       0.000259  0.150000  578.550000\n",
       "prize       0.000259  0.133333  514.266667\n",
       "150p        0.000259  0.090000  347.130000\n",
       "tone        0.000259  0.083333  321.416667\n",
       "guaranteed  0.000259  0.076667  295.703333\n",
       "18          0.000259  0.075000  289.275000\n",
       "www         0.000519  0.135000  260.347500\n",
       "cs          0.000259  0.061667  237.848333\n",
       "1000        0.000259  0.058333  224.991667\n",
       "500         0.000259  0.056667  218.563333\n",
       "awarded     0.000259  0.053333  205.706667\n",
       "uk          0.000519  0.103333  199.278333\n",
       "150ppm      0.000259  0.050000  192.850000\n",
       "000         0.000259  0.041667  160.708333\n",
       "ringtone    0.000259  0.041667  160.708333\n",
       "co          0.000519  0.080000  154.280000\n",
       "mob         0.000259  0.040000  154.280000\n",
       "collection  0.000259  0.038333  147.851667\n",
       "800         0.000259  0.036667  141.423333\n",
       "valid       0.000259  0.036667  141.423333\n",
       "10p         0.000259  0.036667  141.423333\n",
       "5000        0.000259  0.033333  128.566667\n",
       "weekly      0.000259  0.033333  128.566667\n",
       "8007        0.000259  0.033333  128.566667\n",
       "16          0.000519  0.065000  125.352500\n",
       "entry       0.000259  0.031667  122.138333\n",
       "national    0.000259  0.031667  122.138333\n",
       "sae         0.000259  0.031667  122.138333\n",
       "tones       0.000259  0.031667  122.138333\n",
       "land        0.000259  0.030000  115.710000\n",
       "...              ...       ...         ...\n",
       "went        0.012186  0.001667    0.136773\n",
       "gonna       0.012186  0.001667    0.136773\n",
       "dun         0.012445  0.001667    0.133924\n",
       "oh          0.024890  0.003333    0.133924\n",
       "but         0.090744  0.011667    0.128567\n",
       "told        0.013482  0.001667    0.123622\n",
       "gud         0.013741  0.001667    0.121289\n",
       "feel        0.014519  0.001667    0.114792\n",
       "something   0.014519  0.001667    0.114792\n",
       "cos         0.015038  0.001667    0.110833\n",
       "sure        0.015297  0.001667    0.108955\n",
       "said        0.016852  0.001667    0.098897\n",
       "lol         0.016852  0.001667    0.098897\n",
       "morning     0.017112  0.001667    0.097399\n",
       "yeah        0.017112  0.001667    0.097399\n",
       "amp         0.017630  0.001667    0.094534\n",
       "anything    0.017890  0.001667    0.093164\n",
       "doing       0.018927  0.001667    0.088059\n",
       "my          0.151932  0.013333    0.087759\n",
       "already     0.019445  0.001667    0.085711\n",
       "ask         0.019704  0.001667    0.084583\n",
       "way         0.021001  0.001667    0.079362\n",
       "come        0.047446  0.003333    0.070255\n",
       "later       0.030075  0.001667    0.055417\n",
       "da          0.032409  0.001667    0.051427\n",
       "lor         0.032927  0.001667    0.050617\n",
       "she         0.036038  0.001667    0.046247\n",
       "he          0.047187  0.001667    0.035321\n",
       "lt          0.064817  0.001667    0.025713\n",
       "gt          0.065595  0.001667    0.025408\n",
       "\n",
       "[4000 rows x 3 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the DataFrame sorted by spam_ratio\n",
    "tokens.sort_values('spamminess', ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
